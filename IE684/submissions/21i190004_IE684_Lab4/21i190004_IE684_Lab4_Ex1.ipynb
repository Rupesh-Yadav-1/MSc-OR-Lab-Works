{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$\\large\\textbf{Consider the function :}$ \\\\\n",
        "\\begin{align}\n",
        "\\mathbf{f(x)} = 400x_1^2 +0.004x_2^4 \n",
        "  \\\\\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "jWGD09gKOvCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gznWv4EYobKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZoTNwIHcpJkp"
      },
      "outputs": [],
      "source": [
        "#Now we will define a Python function which will compute and return the objective function value \n",
        "def evalf(x):\n",
        "  #Input: x is a numpy array of size 2\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return np.float128(400*x[0]**2 + 0.004*x[1]**4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O1iTOs4BpZwv"
      },
      "outputs": [],
      "source": [
        "#Now we will define a Python function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([800*x[0], 0.016*x[1]**3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G1l7gsROpxxI"
      },
      "outputs": [],
      "source": [
        "#method to find Hessian matrix:\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2\n",
        "\n",
        "  return np.array([[800, 0],[0, 0.048*x[1]**2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ziuAEesIr7li"
      },
      "outputs": [],
      "source": [
        "#method to find inverse of Hessian matrix:\n",
        "def evalh_inv(x):\n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2\n",
        "\n",
        "  return np.linalg.inv(evalh(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GR8xwmye2MHI"
      },
      "outputs": [],
      "source": [
        "#Complete the module to compute the steplength by backtracking without scaling\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  pk=-gradf\n",
        "  while evalf(x+alpha*pk)>evalf(x)+gamma*alpha*np.linalg.multi_dot([gradf,pk]):\n",
        "    alpha=rho*alpha\n",
        "  \n",
        "  return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gyu10EVes4M5"
      },
      "outputs": [],
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, direction, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  while evalf(x+alpha*direction)>evalf(x)+gamma*alpha*np.linalg.multi_dot([gradf,direction]):\n",
        "    alpha=rho*alpha\n",
        "\n",
        "  return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TJ8qsFCDw5Uf"
      },
      "outputs": [],
      "source": [
        "#line search type\n",
        "NEWTONS_CONSTANT_STEP_LENGTH = 1\n",
        "NEWTONS_BACKTRACKING_LINE_SEARCH = 2\n",
        "WITHOUT_SCALING_BACKTRACKING_LINE_SEARCH = 3\n",
        "DIAG_MATRIX_SCALING_BACKTRACKING_LINE_SEARCH = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jtth9FTNAkVR"
      },
      "outputs": [],
      "source": [
        "#code for Newtons method with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_newtons(x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "\n",
        "  g_x = evalg(x)\n",
        "  \n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == NEWTONS_BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "\n",
        "  if line_search_type == NEWTONS_CONSTANT_STEP_LENGTH:\n",
        "    k=0\n",
        "    while (np.linalg.norm(g_x) > tol):\n",
        "      D_k = evalh_inv(x)\n",
        "      direction = -np.matmul(D_k, g_x)\n",
        "      step_length = 1\n",
        "      #implement the gradient descent steps here   \n",
        "      x = np.add(x, np.multiply(step_length,direction)) #update x = x + step_length*direction\n",
        "      k += 1 #increment iteration\n",
        "      g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "  elif line_search_type == NEWTONS_BACKTRACKING_LINE_SEARCH:\n",
        "    k=0\n",
        "    while (np.linalg.norm(g_x) > tol):\n",
        "      D_k = evalh_inv(x)\n",
        "      direction = -np.matmul(D_k, g_x)\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x, g_x, direction, alpha_start, rho, gamma)\n",
        "      #implement the gradient descent steps here   \n",
        "      x = np.add(x, np.multiply(step_length,direction)) #update x = x + step_length*direction\n",
        "      k += 1 #increment iteration\n",
        "      g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "  else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "  return x, evalf(x), k"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 3**"
      ],
      "metadata": {
        "id": "IMPs3OmVQ7eR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1p1Kr1qb78-S"
      },
      "outputs": [],
      "source": [
        "my_start_x = np.array([2.,2.])\n",
        "my_tol= 1e-9\n",
        "\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CphhdiMP8E-b",
        "outputId": "7623f619-5b62-457e-8038-7ea78024cd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Newtons method with constant steplength(=1) :\n",
            "\n",
            "Minimizer value:  [0.         0.00304488]\n",
            "Minimum obj fn value:  3.4382653805813626168e-13\n",
            "Total number of iterations:  16\n"
          ]
        }
      ],
      "source": [
        "min_x1,min_obj1,iters1=find_minimizer_newtons(my_start_x, my_tol, NEWTONS_CONSTANT_STEP_LENGTH)\n",
        "print('For Newtons method with constant steplength(=1) :')\n",
        "print()\n",
        "print('Minimizer value: ',min_x1)\n",
        "print('Minimum obj fn value: ',min_obj1)\n",
        "print('Total number of iterations: ',iters1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_lnus-7AFEi",
        "outputId": "409888d6-5f23-4051-d861-6a4344432f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Newtons method with backtracking line search :\n",
            "\n",
            "Minimizer value:  [0.         0.00304488]\n",
            "Minimum obj fn value:  3.4382653805813626168e-13\n",
            "Total number of iterations:  16\n"
          ]
        }
      ],
      "source": [
        "min_x2,min_obj2,iters2=find_minimizer_newtons(my_start_x, my_tol, NEWTONS_BACKTRACKING_LINE_SEARCH, alpha_start,rho,gamma)\n",
        "print('For Newtons method with backtracking line search :')\n",
        "print()\n",
        "print('Minimizer value: ',min_x2)\n",
        "print('Minimum obj fn value: ',min_obj2)\n",
        "print('Total number of iterations: ',iters2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are getting same solutions within same number of iterations in both cases because after applying backtracking line search we are getting the same steplength(=1). Hence the same solution."
      ],
      "metadata": {
        "id": "BhyiLit1Q9zA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHbOSfv52AFY"
      },
      "source": [
        "**Q 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pvBJLk35y8bn"
      },
      "outputs": [],
      "source": [
        "#The method defines a way to construct D_k matrix used in scaling the gradient in the modified gradient descent scheme\n",
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  H=evalh(x)\n",
        "  return np.array([[1/H[0][0], 0],[0, 1/H[1][1]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1ep6rrKH3zfc"
      },
      "outputs": [],
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_Q4(x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "\n",
        "  g_x = evalg(x)\n",
        "  \n",
        "  alpha_start = args[0]\n",
        "  rho = args[1]\n",
        "  gamma = args[2]\n",
        "\n",
        "  if line_search_type == WITHOUT_SCALING_BACKTRACKING_LINE_SEARCH:\n",
        "    k = 0\n",
        "    while (np.linalg.norm(g_x) > tol):\n",
        "      step_length = compute_steplength_backtracking(x, g_x, alpha_start, rho, gamma)\n",
        "      #implement the gradient descent steps here   \n",
        "      x = np.add(x, np.multiply(step_length,g_x)) #update x = x + step_length*direction\n",
        "      k += 1 #increment iteration\n",
        "      g_x = evalg(x) #compute gradient at new point\n",
        "  \n",
        "  elif line_search_type == DIAG_MATRIX_SCALING_BACKTRACKING_LINE_SEARCH:\n",
        "    k = 0\n",
        "    while (np.linalg.norm(g_x) > tol):\n",
        "      D_k = compute_D_k(x)\n",
        "      direction = -np.matmul(D_k, g_x)\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x, g_x, direction, alpha_start, rho, gamma)\n",
        "      #implement the gradient descent steps here   \n",
        "      x = np.add(x, np.multiply(step_length,direction)) #update x = x + step_length*direction\n",
        "      k += 1 #increment iteration\n",
        "      g_x = evalg(x) #compute gradient at new point\n",
        "  \n",
        "  else:  \n",
        "    raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "  return x, evalf(x), k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rPS6v7GHQL8i"
      },
      "outputs": [],
      "source": [
        "my_start_x = np.array([np.float128(2),np.float128(2)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zVv-iOuN73C5",
        "outputId": "4c208acf-778e-4183-acfa-103582047053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in longdouble_scalars\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0fd5c57a9976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_x3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_obj3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miters3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_minimizer_Q4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWITHOUT_SCALING_BACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Gradient Descent Algorithm with Backtracking line search without scaling :'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minimizer value: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_x3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minimum obj fn value: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_obj3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-55c6968394b8>\u001b[0m in \u001b[0;36mfind_minimizer_Q4\u001b[0;34m(x, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0;31m#implement the gradient descent steps here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#update x = x + step_length*direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e75e8301691d>\u001b[0m in \u001b[0;36mcompute_steplength_backtracking\u001b[0;34m(x, gradf, alpha_start, rho, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mpk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "min_x3,min_obj3,iters3=find_minimizer_Q4(my_start_x, my_tol, WITHOUT_SCALING_BACKTRACKING_LINE_SEARCH, alpha_start,rho,gamma)\n",
        "print('For Gradient Descent Algorithm with Backtracking line search without scaling :')\n",
        "print()\n",
        "print('Minimizer value: ',min_x3)\n",
        "print('Minimum obj fn value: ',min_obj3)\n",
        "print('Total number of iterations: ',iters3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdGaCEssF2Cr",
        "outputId": "6fe40c04-675c-4c28-97f7-cb34dd21d4d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Gradient Descent Algorithm with Backtracking line search with scaling(with help of diagonal matrix) :\n",
            "\n",
            "Minimizer value:  [0.         0.00304488]\n",
            "Minimum obj fn value:  3.4382653805813657382e-13\n",
            "Total number of iterations:  16\n"
          ]
        }
      ],
      "source": [
        "min_x4,min_obj4,iters4=find_minimizer_Q4(my_start_x, my_tol, DIAG_MATRIX_SCALING_BACKTRACKING_LINE_SEARCH, alpha_start,rho,gamma)\n",
        "print('For Gradient Descent Algorithm with Backtracking line search with scaling(with help of diagonal matrix) :')\n",
        "print()\n",
        "print('Minimizer value: ',min_x4)\n",
        "print('Minimum obj fn value: ',min_obj4)\n",
        "print('Total number of iterations: ',iters4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqwG9hDrC5Kw"
      },
      "source": [
        "For Gradient Descent Algorithm with Backtracking line search without scaling the program was not terminating(i ran the code for more than 4 hours and it was still running it is taking no. of iterations of order $10^7$). So, clearly this algorithm is a very bad choice for this problem.\n",
        "\n",
        "While Gradient descent algorithm with backtracking line search with scaling is again giving same result as of methods in **Part 3** of this question because the hessian matrix is coming out to be a diagonal matrix and the inverse of a diagonal matrix is just a diagonal matrix with entries as reciprocal of corrosponding diagonal entries of the matrix. So the inverse of hessian(which is used for scaling in Newtons method with Backtracking line search) is same as Diagonal matrix(which is used for scaling in Gradient descent with scaling with backtracking line search)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "21i190004_IE684_Lab4_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}