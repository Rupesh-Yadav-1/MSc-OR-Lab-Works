{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190004_IE684_Lab1_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "\\begin{equation*}\n",
        "\\begin{split}\n",
        "f(\\mathbf{x}) & =f(x_1,x_2) \\\\\n",
        " & = (x_1+100)^2 + (x_2-25)^2 \\\\\n",
        " & = x_1^2+10000+200x_1+x_2^2+625-50x_2 \\\\\n",
        " & = x_1^2+x_2^2+200x_1-50x_2+10625 \\\\\n",
        " & = [x_1 \\ x_2]\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\begin{bmatrix} 200 & -50 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + 10625 \\\\\n",
        " & = \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c\n",
        "\\end{split}\n",
        "\\end{equation*} \n",
        "\n",
        "\n",
        "So, after comparing we gets, \\\\\n",
        "$\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\mathbf{A}=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\mathbf{b}=\\begin{bmatrix} 200 \\\\ -50 \\end{bmatrix} \\ and \\ \\ c=10625$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "$\\because \\nabla f(x)=\\begin{bmatrix} 2(x_1+100) \\\\ 2(x_2-25) \\end{bmatrix}$\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{split}\n",
        "f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))&=f(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}-\\alpha\\begin{bmatrix} 2(x_1+100) \\\\ 2(x_2-25) \\end{bmatrix}) \\\\\n",
        " & = f(\\begin{bmatrix} x_1-2\\alpha(x_1+100) \\\\ x_2-2\\alpha(x_2-25) \\end{bmatrix}) \\\\\n",
        " & = f(\\begin{bmatrix} x_1(1-2\\alpha)-200\\alpha \\\\ x_2(1-2\\alpha)+50\\alpha \\end{bmatrix}) \\\\\n",
        " &=(x_1(1-2\\alpha)-200\\alpha+100)^2+(x_2(1-2\\alpha)+50\\alpha-25)^2\n",
        "\\end{split}\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Now after differentiating w.r.t $\\alpha$ and equating it with zero, we gets \\\\\n",
        "\\begin{equation*}\n",
        "\\begin{split}\n",
        "2(x_1(1-2\\alpha)-200\\alpha+100)(-2x_1-200)+2(x_2(1-2\\alpha)+50\\alpha-25)(-2x_2+50) & =0 \\\\\n",
        "(1-2\\alpha)((x_1+100)^2+(x_2-25)^2) & =0 \\\\\n",
        "â‡’\\alpha & =0.5\n",
        "\\end{split}\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Since, the function is sum of two square terms. So, function will have minimum at $\\alpha=0.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np\n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\n",
        "def compute_steplength(start_x,grad_value): #add appropriate arguments to the function \n",
        "  step_ln=1\n",
        "  t=10**(-4)\n",
        "  a=0.5\n",
        "\n",
        "  while evalf(start_x-step_ln*grad_value)>(evalf(start_x)-(t*step_ln*np.linalg.multi_dot([grad_value,grad_value]))):\n",
        "    step_ln=a*step_ln\n",
        "  return step_ln "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol): \n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x,g_x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9809bfaa-edd2-4edf-f871-5995ef9941eb"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-100.,   25.]), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_3BxMzfREx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bddd9f-2ce5-4a0f-a6ab-515cb5c92228"
      },
      "source": [
        " t_values=[10**(-(i+1)) for i in range(10)]\n",
        " no_of_iter=[]\n",
        " for i in t_values:\n",
        "   no_of_iter.append(find_minimizer(my_start_x,i)[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(t_values, no_of_iter)\n",
        "# naming the x axis\n",
        "plt.xlabel('tolerance values')\n",
        "# naming the y axis\n",
        "plt.ylabel('no. of iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "-jJ-H9x5Px8Y",
        "outputId": "0503412b-3ee3-440f-b0e8-48b3bcb12738"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWTklEQVR4nO3dfZQldX3n8feHmQEU5HFGFhlgACE6uJiQBlFUWDYqqMCKrmjiA5p1EgUfzi4aWPWgJJ4koJuNMYKTHARMVlTUCGoEBBHWyEqPwMBAMCNqmNGVAQREsiL43T9utV6amu470119e7rfr3PuuVW/erjfXzf0Z6p+datSVUiSNN5Wwy5AkjQ7GRCSpFYGhCSplQEhSWplQEiSWi0cdgHTZfHixbVs2bJhlyFJW5RVq1bdXVVL2pbNmYBYtmwZo6Ojwy5DkrYoSX6wsWWeYpIktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTKgJAkteosIJKcl+SuJLdsZHmSfDjJ2iSrkxw8bvkOSdYl+UhXNUqSNq7LI4jzgaMnWH4MsH/zWgGcM275HwPXdFKZJGlSnQVEVV0D3DvBKscDF1bPdcBOSXYHSPLbwG7A5V3VJ0ma2DDHIPYA7uybXwfskWQr4EPAqZPtIMmKJKNJRjds2NBRmZI0P83GQeq3AF+uqnWTrVhVK6tqpKpGlixZMgOlSdL8sXCIn70e2LNvfmnT9mzgeUneAmwPbJ3kwao6bQg1StK8NcyAuAQ4JclFwLOA+6vqR8Dvja2Q5CRgxHCQpJnXWUAk+SRwJLA4yTrgDGARQFWdC3wZeDGwFngIeENXtUiSNl1nAVFVr55keQEnT7LO+fQul5UkzbDZOEgtSZoFDAhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwISVKrzgIiyXlJ7kpyy0aWJ8mHk6xNsjrJwU37byb5ZpI1TfuJXdUoSdq4Lo8gzgeOnmD5McD+zWsFcE7T/hDwuqo6sNn+fybZqcM6JUktFna146q6JsmyCVY5Hriwqgq4LslOSXavqu/07eOHSe4ClgD3dVWrJOnxhjkGsQdwZ9/8uqbtV5IcCmwNfHcG65IkMYsHqZPsDnwCeENV/XIj66xIMppkdMOGDTNboCTNcZMGRJLtkmzVTB+Q5Lgki6bhs9cDe/bNL23aSLID8CXg3VV13cZ2UFUrq2qkqkaWLFkyDSVJksYMcgRxDbBtkj2Ay4HX0huAnqpLgNc1VzMdBtxfVT9KsjXweXrjExdPw+dIkjbDIIPUqaqHkvw+8NGqOivJjZNulHwSOBJYnGQdcAawCKCqzgW+DLwYWEvvyqU3NJu+Eng+sGuSk5q2k6pq0s+UJE2fgQIiybOB3wN+v2lbMNlGVfXqSZYXcHJL+98BfzdAXZKkDg1yiuntwOnA56tqTZJ9ga91W5YkadgmPYKoqmvojUOMzd8BvK3LoiRJwzdpQCQ5ADgVWNa/flUd1V1ZkqRhG2QM4jPAucDfAo92W44kabYYJCAeqapzJl9NkjSXDDJIfWmStyTZPckuY6/OK5MkDdUgRxCvb97f2ddWwL7TX44kabYY5CqmfWaiEEnS7DLIVUyLgDfT+3YzwNXAx6rqFx3WJUkaskFOMZ1D7xYZH23mX9u0/ZeuipIkDd8gAXFIVT2zb/6qJDd1VZAkaXYY5CqmR5PsNzbT3GrD70NI0hw3yBHEO4GvJbkDCLA3v77zqiRpjhrkKqYrk+wP/EbTdHtV/bzbsiRJw7bRgEhyVFVdleSEcYuemoSq+lzHtUmShmiiI4gjgKuAY1uWFWBASNIcttGAqKozmskzq+p7/cuS+OU5SZrjBrmK6bMtbT4rWpLmuInGIJ4GHAjsOG4cYgdg264LkyQN10RjEL8BvBTYiceOQ/wUeFOXRUmShm+iMYgvAF9I8uyq+uYM1iRJmgUG+aLcDUlOpne66VenlqrqjZ1VJUkaukEGqT8B/DvgRcDXgaX0TjNJkuawQQLiqVX1XuBnVXUB8BLgWd2WJUkatkECYuy5D/cleQawI/Dk7kqSJM0Gg4xBrEyyM/Ae4BJge+C9nVYlSRq6CQMiyVbAA1X1E+AafA61JM0bE55iqqpfAu+aoVokSbPIIGMQX01yapI9k+wy9uq8MknSUA0yBnFi835yX1vh6SZJmtMGeWCQd26VpHlo0lNMSZ6Y5D1JVjbz+yd56QDbnZfkriS3bGR5knw4ydokq5Mc3Lfs9Un+pXm9flM6JEmaHoOMQXwceBh4TjO/HviTAbY7Hzh6guXHAPs3rxXAOQDN+MYZ9L6MdyhwRnOZrSRpBg0yBrFfVZ2Y5NUAVfVQkky2UVVdk2TZBKscD1xYVQVcl2SnJLsDRwJXVNW9AEmuoBc0nxyg1s3y/kvXcOsPH+hq95LUqeVP2YEzjj1w2vc7yBHEw0meQG9gmiT7AT+fhs/eA7izb35d07ax9sdJsiLJaJLRDRs2TENJkqQxgxxBvA/4CrBnkr8HDgfe0GVRg6qqlcBKgJGRkdrc/XSRvJK0pRvkKqbLk6wCDgMCvL2q7p6Gz14P7Nk3v7RpW0/vNFN/+9XT8HmSpE0wyFVMV1bVPVX1par6YlXdneTKafjsS4DXNVczHQbcX1U/Ai4DXphk52Zw+oVNmyRpBk30TOptgScCi5s/1GMD0zuwkTGBcdt/kt6RwOIk6+hdmbQIoKrOBb4MvBhYCzxEc9qqqu5N8sfA9c2uzhwbsJYkzZyJTjH9AfAO4CnAt/vaHwA+MtmOq+rVkywvHvvt7P5l5wHnTfYZkqTuTPRM6r8E/jLJW6vqr2awJknSLDDRKaajquoqYH2SE8Yvr6rPdVqZJGmoJjrFdARwFXBsy7ICDAhJmsMmOsV0RvM+K77zIEmaWYN8k1qSNA8ZEJKkVhsNiCT/uXn3eRCSNA9NdARxevP+2ZkoRJI0u0x0FdM9SS4H9klyyfiFVXVcd2VJkoZtooB4CXAw8AngQzNTjiRptpjoMteH6T3I5zlVtSHJ9k37gzNWnSRpaAa5imm3JDcAa4Bbk6xK8oyO65IkDdkgAbES+K9VtXdV7QX8t6ZNkjSHDRIQ21XV18ZmqupqYLvOKpIkzQqDPHL0jiTvpTdYDfAa4I7uSpIkzQaDHEG8EVhC7+Z8nwUWN22SpDlskGdS/wR42wzUIkmaRbwXkySplQEhSWplQEiSWm1WQCR56XQXIkmaXTb3COKQaa1CkjTrbFZAjD2OVJI0d00aEEl2TPIXSUab14eS7DgTxUmShmeQI4jzgAeAVzavB4CPd1mUJGn4BrnVxn5V9fK++fcnubGrgiRJs8MgRxD/luS5YzNJDgf+rbuSJEmzwSBHEH8IXNiMOwS4Fzipy6IkScM3yL2YbgKemWSHZv6BzquSJA3dpAGRZBvg5cAyYGESAKrqzE4rkyQN1SBjEF8AjgceAX7W95pUkqOT3J5kbZLTWpbvneTKJKuTXJ1kad+ys5KsSXJbkg9nLJkkSTNikDGIpVV19KbuOMkC4K+BFwDrgOuTXFJVt/at9kHgwqq6IMlRwJ8Cr03yHOBw4KBmvf8NHAFcval1SJI2zyBHEP+U5N9vxr4PBdZW1R1V9TBwEb0jkX7Lgaua6a/1LS9gW2BrYBtgEfDjzahBkrSZBgmI5wKrmlNFq5PcnGT1ANvtAdzZN7+uaet3E3BCM/0y4ElJdq2qb9ILjB81r8uq6rYBPlOSNE0GOcV0TIeffyrwkSQnAdcA64FHkzwVeDowNiZxRZLnVdW1/RsnWQGsANhrr706LFOS5p9BLnP9wWbuez2wZ9/80qatf98/pDmCSLI98PKqui/Jm4DrqurBZtk/As8Grh23/UpgJcDIyEhtZp2SpBZdPjDoemD/JPsk2Rp4FXBJ/wpJFicZq+F0evd9AvhX4IgkC5MsojdA7SkmSZpBnQVEVT0CnAJcRu+P+6erak2SM5Mc16x2JHB7ku8AuwEfaNovBr4L3ExvnOKmqrq0q1olSY+XqrlxZmZkZKRGR0eHXYYkbVGSrKqqkbZlPpNaktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrToNiCRHJ7k9ydokp7Us3zvJlUlWJ7k6ydK+ZXsluTzJbUluTbKsy1olSY/VWUAkWQD8NXAMsBx4dZLl41b7IHBhVR0EnAn8ad+yC4Gzq+rpwKHAXV3VKkl6vC6PIA4F1lbVHVX1MHARcPy4dZYDVzXTXxtb3gTJwqq6AqCqHqyqhzqsVZI0TpcBsQdwZ9/8uqat303ACc30y4AnJdkVOAC4L8nnktyQ5OzmiOQxkqxIMppkdMOGDR10QZLmr2EPUp8KHJHkBuAIYD3wKLAQeF6z/BBgX+Ck8RtX1cqqGqmqkSVLlsxY0ZI0H3QZEOuBPfvmlzZtv1JVP6yqE6rqt4B3N2330TvauLE5PfUI8A/AwR3WKkkap8uAuB7YP8k+SbYGXgVc0r9CksVJxmo4HTivb9udkowdFhwF3NphrZKkcToLiOZf/qcAlwG3AZ+uqjVJzkxyXLPakcDtSb4D7AZ8oNn2UXqnl65McjMQ4G+6qlWS9HipqmHXMC1GRkZqdHR02GVI0hYlyaqqGmlbNuxBaknSLGVASJJaGRCSpFYGhCSplQEhSWplQEiSWhkQkqRWBoQkqZUBIUlqZUBIkloZEJKkVgaEJKmVASFJamVASJJaGRCSpFYGhCSplQEhSWplQEiSWhkQkqRWBoQkqZUBIUlqZUBIkloZEJKkVgaEJKlVqmrYNUyLJBuAH0xhF4uBu6epnC3FfOvzfOsv2Of5Yip93ruqlrQtmDMBMVVJRqtqZNh1zKT51uf51l+wz/NFV332FJMkqZUBIUlqZUD82sphFzAE863P862/YJ/ni0767BiEJKmVRxCSpFYGhCSp1ZwPiCRHJ7k9ydokp7Us3ybJp5rl/yfJsr5lpzfttyd50UzWPRWb2+ckL0iyKsnNzftRM1375prK77lZvleSB5OcOlM1T9UU/9s+KMk3k6xpft/bzmTtm2sK/20vSnJB09fbkpw+07VvrgH6/Pwk307ySJJXjFv2+iT/0rxev8kfXlVz9gUsAL4L7AtsDdwELB+3zluAc5vpVwGfaqaXN+tvA+zT7GfBsPvUcZ9/C3hKM/0MYP2w+9N1n/uWXwx8Bjh12P2Zgd/zQmA18Mxmftd58N/27wIXNdNPBL4PLBt2n6apz8uAg4ALgVf0te8C3NG879xM77wpnz/XjyAOBdZW1R1V9TBwEXD8uHWOBy5opi8G/mOSNO0XVdXPq+p7wNpmf7PdZve5qm6oqh827WuAJyTZZkaqnpqp/J5J8p+A79Hr85ZiKn1+IbC6qm4CqKp7qurRGap7KqbS5wK2S7IQeALwMPDAzJQ9JZP2uaq+X1WrgV+O2/ZFwBVVdW9V/QS4Ajh6Uz58rgfEHsCdffPrmrbWdarqEeB+ev+iGmTb2Wgqfe73cuDbVfXzjuqcTpvd5yTbA38EvH8G6pxOU/k9HwBUksuaUxPvmoF6p8NU+nwx8DPgR8C/Ah+sqnu7LngaTOXv0JT/hi3clJU1PyQ5EPhzev/SnOveB/xFVT3YHFDMBwuB5wKHAA8BVyZZVVVXDresTh0KPAo8hd7plmuTfLWq7hhuWbPbXD+CWA/s2Te/tGlrXac5/NwRuGfAbWejqfSZJEuBzwOvq6rvdl7t9JhKn58FnJXk+8A7gP+e5JSuC54GU+nzOuCaqrq7qh4Cvgwc3HnFUzeVPv8u8JWq+kVV3QV8A9gS7tc0lb9DU/8bNuxBmI4HeBbSG5jZh18P8Bw4bp2Teeyg1qeb6QN57CD1HWwZA3lT6fNOzfonDLsfM9Xnceu8jy1nkHoqv+edgW/TG6xdCHwVeMmw+9Rxn/8I+HgzvR1wK3DQsPs0HX3uW/d8Hj9I/b3m971zM73LJn3+sH8AM/ADfjHwHXpXAry7aTsTOK6Z3pbe1StrgW8B+/Zt++5mu9uBY4bdl677DLyH3nnaG/teTx52f7r+PfftY4sJiKn2GXgNvUH5W4Czht2XrvsMbN+0r2nC4Z3D7ss09vkQekeFP6N3tLSmb9s3Nj+LtcAbNvWzvdWGJKnVXB+DkCRtJgNCktTKgJAktTIgJEmtDAhJUisDQlu0JDslecsA6y1LcstM1DRTklydZEv4spe2UAaEtnQ70buD57RqvoUrzWsGhLZ0fwbsl+TGJGen5+wktzT3/j9x/AZJFjTrXJ9kdZI/aNqPTHJtkkvofZmKJP/QPBtjTZIVfft4MMkHktyU5LokuzXtuyX5fNN+U5LnNO2vSfKtps6PJVkwrqajk3ymb/7IJF9sps9JMtrU0HpTwSQP9k2/Isn5zfSSJJ9t+np9ksOb9iOaWm5MckOSJ23WT19z27C/JejL11Re9O6Ff0vf/Mvp3dZ4AbAbvTt37t6/HrACeE8zvQ0wSu9WBkfS+zbqPn3726V5fwK9bx3v2swXcGwzfVbf/j4FvKOZXkDvXkBPBy4FFjXtH6V3r6v+fixsat2umT8HeM24GhYAV9PcIqKZHmmmH+zb1yuA85vp/wU8t5neC7itmb4UOLyZ3h5YOOzfpa/Z9/IwWnPNc4FPVu/5Bj9O8nV6tyJY3bfOC4GD+p6+tSOwP71nBHyres//GPO2JC9rpvds1runWfeLTfsq4AXN9FHA6wCaGu5P8lrgt4HrmzvGPgG4q7/oqnokyVeAY5NcDLwEGLsN9yubo5eF9MJu+bj+TOR3gOV9d6rdobnF+TeA/5Hk74HPVdW6AfenecSA0HwU4K1VddljGpMj6R1B9M//DvDsqnooydX07vUD8IuqGrtPzaNM/P9SgAuqarLHXF4EnALcC4xW1U+T7AOcChxSVT9pTh21PR60/545/cu3Ag6rqv83bv0/S/Ilevf5+UaSF1XVP09Sn+YZxyC0pfsp0H/+/FrgxGacYQnwfHo3bet3GfDmJIsAkhyQZLuWfe8I/KQJh6cBhw1Qz5XAm5v9LkiyY9P2iiRPbtp3SbJ3y7Zfp3fb7TfRCwuAHeiF1v3NOMcxG/ncHyd5epKtgJf1tV8OvHVsJslvNu/7VdXNVfXnwPXA0wbom+YZA0JbtKq6h96/gG9Jcja9Z1mspndb5KuAd1XV/x232d/SG4T+dnPp68doPwL4CrAwyW30BsOvG6CktwP/IcnN9E49La+qW+ndKffyJKvpjZHs3tKXR+mdtjqmead6jwW9AfhneuMJ39jI557WbPNP9J6aNuZtwEgzGH8r8IdN+zuan9lq4BfAPw7QN80z3s1VktTKIwhJUisDQpLUyoCQJLUyICRJrQwISVIrA0KS1MqAkCS1+v9iMLT7cOdY8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case of fixed step length value the function was taking large number of iterations but here when we found step length with line search method it came out to be ideal step length and minimum was found within a single iteration only."
      ],
      "metadata": {
        "id": "rT0GDfBWliqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y_KchIZhegMP"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}